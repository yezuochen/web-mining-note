{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPWm4w6Qel32Fd9XCiMQ9TH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Deep Learning for Natural Language Processing\n","##Chapter 10  Applications of Transformers: Hands-on with BERT"],"metadata":{"id":"sY8KuwhtowLF"}},{"cell_type":"markdown","source":["BERT in TensorFlow Hub: https://tfhub.dev/google/collections/bert/1"],"metadata":{"id":"WzN8AN4opPOx"}},{"cell_type":"code","source":["import tensorflow as tf\n","#!pip install keras-bert\n","from keras_bert import gen_batch_inputs, get_base_dict\n","from tensorflow import keras\n","from keras_bert import get_model, compile_model\n","import numpy as np"],"metadata":{"id":"VHKUbYsxqusm","executionInfo":{"status":"ok","timestamp":1696863297076,"user_tz":-480,"elapsed":8,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n","from transformers import TFBertModel, BertTokenizer\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5mMeCyZ1agD","executionInfo":{"status":"ok","timestamp":1696861677485,"user_tz":-480,"elapsed":24558,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}},"outputId":"edbb5e62-d2b9-4021-df5d-703984832708"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"]}]},{"cell_type":"markdown","source":["###Listing 10.1 A dedicated Keras layer for BERT models"],"metadata":{"id":"8jcT5nV6p2Ht"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"8sA6fXy4otQo","executionInfo":{"status":"ok","timestamp":1696858849400,"user_tz":-480,"elapsed":323,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}}},"outputs":[],"source":["class BertLayer(tf.keras.layers.Layer):\n","\n","    def __init__(\n","        self,\n","        n_fine_tune_layers=12,\n","        bert_path=\n","        \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n","        **kwargs\n","    ):\n","        self.n_fine_tune_layers = n_fine_tune_layers\n","        self.trainable = True\n","        self.output_size = 768\n","        self.bert_path = bert_path\n","\n","        super(BertLayer, self).__init__(**kwargs)\n","\n","\n","    def build(self, input_shape):\n","        self.bert = hub.Module(\n","            self.bert_path,\n","            trainable=self.trainable,\n","            name=f\"{self.name}_module\"\n","        )\n","        trainable_vars = self.bert.variables\n","        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n","        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n","\n","        for var in trainable_vars:\n","            self._trainable_weights.append(var)\n","\n","        for var in self.bert.variables:\n","            if var not in self._trainable_weights:\n","                self._non_trainable_weights.append(var)\n","\n","        super(BertLayer, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n","\n","        input_ids, input_mask, segment_ids = inputs\n","\n","        bert_inputs = dict(\n","            input_ids=input_ids, input_mask=input_mask,\n","            segment_ids=segment_ids\n","        )\n","        result = self.bert(inputs=bert_inputs, signature=\"tokens\",\n","        as_dict=True)[\n","            \"sequence_output\"\n","        ]\n","        return result\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], self.output_size)"]},{"cell_type":"markdown","source":["###Listing 10.2 Processing input data for BERT"],"metadata":{"id":"knQUgWMkq7_Q"}},{"cell_type":"code","source":["def readSentencePairs(fn):\n","    with open(fn) as f:\n","        lines = f.readlines()\n","\n","    pairs=zip(lines, lines[1:])\n","    paired_sentences=[[a.rstrip().split(),b.rstrip().split()]\n","    for (a,b) in pairs]\n","\n","    tokenD = get_base_dict()\n","\n","    for pairs in paired_sentences:\n","        for token in pairs[0] + pairs[1]:\n","            if token not in tokenD:\n","                tokenD[token] = len(tokenD)\n","    tokenL = list(tokenD.keys())\n","    return (paired_sentences,tokenD,tokenL)"],"metadata":{"id":"FbgyIqFvq9VP","executionInfo":{"status":"ok","timestamp":1696858926585,"user_tz":-480,"elapsed":7,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["###Listing 10.3 Generating batch data for BERT"],"metadata":{"id":"kYkwAm69rg3N"}},{"cell_type":"code","source":["# use !pip install keras-bert\n","def BertGenerator(paired_sentences, tokenD, tokenL):\n","    while True:\n","        yield gen_batch_inputs(\n","            paired_sentences,\n","            tokenD,\n","            tokenL,\n","            seq_len=200,\n","            mask_rate=0.3,\n","            swap_sentence_rate=0.5,\n","        )"],"metadata":{"id":"TGssM2yvriLI","executionInfo":{"status":"ok","timestamp":1696859141382,"user_tz":-480,"elapsed":17,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["###Listing 10.4 Training a proprietary BERT model on data"],"metadata":{"id":"eN2VFCJssQuu"}},{"cell_type":"code","source":["def buildBertModel(paired_sentences,tokenD,tokenL, model_path):\n","    model = get_model(\n","        token_num=len(tokenD),\n","        head_num=5,\n","        transformer_num=12,\n","        embed_dim=256,\n","        feed_forward_dim=100,\n","        seq_len=200,\n","        pos_num=200,\n","        dropout_rate=0.05\n","    )\n","    compile_model(model)\n","\n","    model.fit_generator(\n","        generator=BertGenerator(paired_sentences,tokenD,tokenL),\n","        steps_per_epoch=100,\n","        epochs=10\n","    )\n","    model.save(model_path)"],"metadata":{"id":"MIQ7vJjYsRtB","executionInfo":{"status":"ok","timestamp":1696859598696,"user_tz":-480,"elapsed":305,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# buildBertModel2 is modified by ChatGPT from buildBertModel\n","def buildBertModel2(paired_sentences, tokenD, tokenL, model_path):\n","    # Load a pre-trained BERT model and tokenizer\n","    model_name = \"bert-base-uncased\"\n","    tokenizer = BertTokenizer.from_pretrained(model_name)\n","    bert_model = TFBertModel.from_pretrained(model_name)\n","\n","    # Define your custom layers for fine-tuning\n","    input_ids = Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n","    outputs = bert_model(input_ids)\n","    pooled_output = outputs[1]  # Use the pooled output for classification\n","    dense_layer = Dense(2, activation=\"softmax\")(pooled_output)  # Example: Binary classification\n","\n","    # Create the custom model\n","    custom_model = Model(inputs=input_ids, outputs=dense_layer)\n","\n","    # Compile the model\n","    optimizer = Adam(learning_rate=1e-5)\n","    custom_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","    # Tokenize your input data (paired_sentences) and prepare it for training\n","    input_data = tokenizer(paired_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n","    labels = tokenL  # Assuming tokenL contains labels for classification\n","\n","    # Train your model\n","    custom_model.fit(input_data, labels, epochs=10, batch_size=32)  # Adjust batch_size as needed\n","\n","    # Save the model\n","    custom_model.save(model_path)\n","\n","# Example usage:\n","# buildBertModel(paired_sentences, tokenD, tokenL, \"./bert_model\")"],"metadata":{"id":"r_CDJgt21r7D","executionInfo":{"status":"ok","timestamp":1696862802688,"user_tz":-480,"elapsed":347,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2baMBKwVwjVO","executionInfo":{"status":"ok","timestamp":1696860510349,"user_tz":-480,"elapsed":460,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}},"outputId":"ec2edfe6-cd2d-463c-f3e6-8278a8f446dc"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["#upload The Cask of Amontillado.txt as sample\n","sentences=\"The Cask of Amontillado.txt\"\n","(paired_sentences,tokenD,tokenL)=readSentencePairs(sentences)\n","\n","print(paired_sentences)\n","print(tokenD)\n","print(tokenL)\n","print(len(tokenL))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5XO6DX5gtrtn","executionInfo":{"status":"ok","timestamp":1696863413683,"user_tz":-480,"elapsed":9,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}},"outputId":"3e04b2f4-6be1-4d51-a4e1-1abb00118740"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["[[['\"The', 'Cask', 'of', 'Amontillado\"', 'is', 'a', 'short', 'story', 'by', 'Edgar', 'Allan', 'Poe.', 'It', 'tells', 'the', 'tale', 'of', 'Montresor,', 'who', 'seeks', 'revenge', 'against', 'Fortunato', 'for', 'some', 'unknown', 'offense.', 'Montresor', 'is', 'a', 'cunning', 'character', 'who', 'lures', 'Fortunato', 'into', 'the', 'catacombs', 'of', 'his', 'family', 'estate.'], []], [[], ['Fortunato', 'is', 'known', 'for', 'his', 'expertise', 'in', 'wine.', 'He', 'takes', 'pride', 'in', 'his', 'connoisseurship', 'and', 'is', 'always', 'on', 'the', 'lookout', 'for', 'rare', 'and', 'valuable', 'wines.', 'This', 'weakness', 'becomes', 'his', 'downfall', 'when', 'Montresor', 'uses', \"Fortunato's\", 'passion', 'for', 'wine', 'to', 'trap', 'him.']], [['Fortunato', 'is', 'known', 'for', 'his', 'expertise', 'in', 'wine.', 'He', 'takes', 'pride', 'in', 'his', 'connoisseurship', 'and', 'is', 'always', 'on', 'the', 'lookout', 'for', 'rare', 'and', 'valuable', 'wines.', 'This', 'weakness', 'becomes', 'his', 'downfall', 'when', 'Montresor', 'uses', \"Fortunato's\", 'passion', 'for', 'wine', 'to', 'trap', 'him.'], []], [[], ['As', 'Fortunato', 'ventures', 'deeper', 'into', 'the', 'catacombs,', 'he', 'becomes', 'more', 'intoxicated', 'and', 'less', 'aware', 'of', 'his', 'surroundings.', 'He', 'follows', 'Montresor', 'through', 'the', 'winding', 'passages,', 'unaware', 'of', 'the', 'danger', 'that', 'awaits', 'him.']], [['As', 'Fortunato', 'ventures', 'deeper', 'into', 'the', 'catacombs,', 'he', 'becomes', 'more', 'intoxicated', 'and', 'less', 'aware', 'of', 'his', 'surroundings.', 'He', 'follows', 'Montresor', 'through', 'the', 'winding', 'passages,', 'unaware', 'of', 'the', 'danger', 'that', 'awaits', 'him.'], []], [[], ['The', 'story', 'reaches', 'its', 'climax', 'when', 'Montresor', 'chains', 'Fortunato', 'to', 'a', 'wall', 'and', 'proceeds', 'to', 'brick', 'him', 'into', 'a', 'niche', 'in', 'the', 'catacombs.', \"Fortunato's\", 'pleas', 'for', 'mercy', 'go', 'unanswered,', 'and', 'he', 'is', 'entombed', 'alive', 'by', 'his', 'vengeful', 'adversary.']], [['The', 'story', 'reaches', 'its', 'climax', 'when', 'Montresor', 'chains', 'Fortunato', 'to', 'a', 'wall', 'and', 'proceeds', 'to', 'brick', 'him', 'into', 'a', 'niche', 'in', 'the', 'catacombs.', \"Fortunato's\", 'pleas', 'for', 'mercy', 'go', 'unanswered,', 'and', 'he', 'is', 'entombed', 'alive', 'by', 'his', 'vengeful', 'adversary.'], []], [[], ['\"The', 'Cask', 'of', 'Amontillado\"', 'is', 'a', 'chilling', 'tale', 'of', 'revenge', 'and', 'betrayal.', 'It', 'showcases', 'Edgar', 'Allan', \"Poe's\", 'mastery', 'of', 'suspense', 'and', 'psychological', 'horror,', 'making', 'it', 'one', 'of', 'his', 'most', 'famous', 'and', 'enduring', 'works.']]]\n","{'': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, '\"The': 5, 'Cask': 6, 'of': 7, 'Amontillado\"': 8, 'is': 9, 'a': 10, 'short': 11, 'story': 12, 'by': 13, 'Edgar': 14, 'Allan': 15, 'Poe.': 16, 'It': 17, 'tells': 18, 'the': 19, 'tale': 20, 'Montresor,': 21, 'who': 22, 'seeks': 23, 'revenge': 24, 'against': 25, 'Fortunato': 26, 'for': 27, 'some': 28, 'unknown': 29, 'offense.': 30, 'Montresor': 31, 'cunning': 32, 'character': 33, 'lures': 34, 'into': 35, 'catacombs': 36, 'his': 37, 'family': 38, 'estate.': 39, 'known': 40, 'expertise': 41, 'in': 42, 'wine.': 43, 'He': 44, 'takes': 45, 'pride': 46, 'connoisseurship': 47, 'and': 48, 'always': 49, 'on': 50, 'lookout': 51, 'rare': 52, 'valuable': 53, 'wines.': 54, 'This': 55, 'weakness': 56, 'becomes': 57, 'downfall': 58, 'when': 59, 'uses': 60, \"Fortunato's\": 61, 'passion': 62, 'wine': 63, 'to': 64, 'trap': 65, 'him.': 66, 'As': 67, 'ventures': 68, 'deeper': 69, 'catacombs,': 70, 'he': 71, 'more': 72, 'intoxicated': 73, 'less': 74, 'aware': 75, 'surroundings.': 76, 'follows': 77, 'through': 78, 'winding': 79, 'passages,': 80, 'unaware': 81, 'danger': 82, 'that': 83, 'awaits': 84, 'The': 85, 'reaches': 86, 'its': 87, 'climax': 88, 'chains': 89, 'wall': 90, 'proceeds': 91, 'brick': 92, 'him': 93, 'niche': 94, 'catacombs.': 95, 'pleas': 96, 'mercy': 97, 'go': 98, 'unanswered,': 99, 'entombed': 100, 'alive': 101, 'vengeful': 102, 'adversary.': 103, 'chilling': 104, 'betrayal.': 105, 'showcases': 106, \"Poe's\": 107, 'mastery': 108, 'suspense': 109, 'psychological': 110, 'horror,': 111, 'making': 112, 'it': 113, 'one': 114, 'most': 115, 'famous': 116, 'enduring': 117, 'works.': 118}\n","['', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '\"The', 'Cask', 'of', 'Amontillado\"', 'is', 'a', 'short', 'story', 'by', 'Edgar', 'Allan', 'Poe.', 'It', 'tells', 'the', 'tale', 'Montresor,', 'who', 'seeks', 'revenge', 'against', 'Fortunato', 'for', 'some', 'unknown', 'offense.', 'Montresor', 'cunning', 'character', 'lures', 'into', 'catacombs', 'his', 'family', 'estate.', 'known', 'expertise', 'in', 'wine.', 'He', 'takes', 'pride', 'connoisseurship', 'and', 'always', 'on', 'lookout', 'rare', 'valuable', 'wines.', 'This', 'weakness', 'becomes', 'downfall', 'when', 'uses', \"Fortunato's\", 'passion', 'wine', 'to', 'trap', 'him.', 'As', 'ventures', 'deeper', 'catacombs,', 'he', 'more', 'intoxicated', 'less', 'aware', 'surroundings.', 'follows', 'through', 'winding', 'passages,', 'unaware', 'danger', 'that', 'awaits', 'The', 'reaches', 'its', 'climax', 'chains', 'wall', 'proceeds', 'brick', 'him', 'niche', 'catacombs.', 'pleas', 'mercy', 'go', 'unanswered,', 'entombed', 'alive', 'vengeful', 'adversary.', 'chilling', 'betrayal.', 'showcases', \"Poe's\", 'mastery', 'suspense', 'psychological', 'horror,', 'making', 'it', 'one', 'most', 'famous', 'enduring', 'works.']\n","119\n"]}]},{"cell_type":"code","source":["#upload The Cask of Amontillado.txt as sample\n","sentences1=\"The Cask of Amontillado1.txt\"\n","(paired_sentences1,tokenD,tokenL)=readSentencePairs(sentences1)\n","\n","print(paired_sentences1)\n","print(tokenD)\n","print(tokenL)\n","print(len(tokenL))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2HrdubZb9UDM","executionInfo":{"status":"ok","timestamp":1696863717493,"user_tz":-480,"elapsed":322,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}},"outputId":"aa5bb002-514b-41c5-e9a5-8cda4eb3f4bf"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["[[['He', 'had', 'a', 'weak', 'point', '—', 'this', 'Fortunato', '—', 'although', 'in', 'other', 'regards', 'he', 'was', 'a', 'man', 'to', 'be', 'respected', 'and', 'even', 'feared.', 'He', 'prided', 'himself', 'on', 'his', 'connoisseurship', 'in', 'wine.', 'Few', 'Italians', 'have', 'the', 'true', 'virtuoso', 'spirit.', 'For', 'the', 'most', 'part', 'their', 'enthusiasm', 'is', 'adopted', 'to'], ['suit', 'the', 'time', 'and', 'opportunity—to', 'practise', 'imposture', 'upon', 'the', 'British', 'and', 'Austrian.']], [['suit', 'the', 'time', 'and', 'opportunity—to', 'practise', 'imposture', 'upon', 'the', 'British', 'and', 'Austrian.'], ['millionaires']]]\n","{'': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, 'He': 5, 'had': 6, 'a': 7, 'weak': 8, 'point': 9, '—': 10, 'this': 11, 'Fortunato': 12, 'although': 13, 'in': 14, 'other': 15, 'regards': 16, 'he': 17, 'was': 18, 'man': 19, 'to': 20, 'be': 21, 'respected': 22, 'and': 23, 'even': 24, 'feared.': 25, 'prided': 26, 'himself': 27, 'on': 28, 'his': 29, 'connoisseurship': 30, 'wine.': 31, 'Few': 32, 'Italians': 33, 'have': 34, 'the': 35, 'true': 36, 'virtuoso': 37, 'spirit.': 38, 'For': 39, 'most': 40, 'part': 41, 'their': 42, 'enthusiasm': 43, 'is': 44, 'adopted': 45, 'suit': 46, 'time': 47, 'opportunity—to': 48, 'practise': 49, 'imposture': 50, 'upon': 51, 'British': 52, 'Austrian.': 53, 'millionaires': 54}\n","['', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'He', 'had', 'a', 'weak', 'point', '—', 'this', 'Fortunato', 'although', 'in', 'other', 'regards', 'he', 'was', 'man', 'to', 'be', 'respected', 'and', 'even', 'feared.', 'prided', 'himself', 'on', 'his', 'connoisseurship', 'wine.', 'Few', 'Italians', 'have', 'the', 'true', 'virtuoso', 'spirit.', 'For', 'most', 'part', 'their', 'enthusiasm', 'is', 'adopted', 'suit', 'time', 'opportunity—to', 'practise', 'imposture', 'upon', 'British', 'Austrian.', 'millionaires']\n","55\n"]}]},{"cell_type":"code","source":["model_path=\"./bert.model\"\n","paired_sentences1 = paired_sentences1[0]\n","buildBertModel2(paired_sentences1,tokenD,tokenL,model_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"id":"XevhsRD9vGyK","executionInfo":{"status":"error","timestamp":1696863799623,"user_tz":-480,"elapsed":12213,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}},"outputId":"181a0bf7-a2fb-4540-a3af-b9177dcba78e"},"execution_count":89,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-89-fc367012f7eb>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./bert.model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpaired_sentences1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaired_sentences1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbuildBertModel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaired_sentences1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-68-93f5db7b4131>\u001b[0m in \u001b[0;36mbuildBertModel2\u001b[0;34m(paired_sentences, tokenD, tokenL, model_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Tokenize your input data (paired_sentences) and prepare it for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaired_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenL\u001b[0m  \u001b[0;31m# Assuming tokenL contains labels for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2890\u001b[0m                 )\n\u001b[1;32m   2891\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2892\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2893\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3081\u001b[0m         )\n\u001b[1;32m   3082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3083\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3084\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"cell_type":"code","source":["print(type(paired_sentences))\n","print(type(paired_sentences[1]))\n","print(type(paired_sentences[1][1]))\n","print(type(paired_sentences[1][1][1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wyqkhw4O2HVw","executionInfo":{"status":"ok","timestamp":1696863416806,"user_tz":-480,"elapsed":333,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}},"outputId":"c1bd6634-f892-4112-9200-d8ad7c019340"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'>\n","<class 'list'>\n","<class 'list'>\n","<class 'str'>\n"]}]},{"cell_type":"code","source":["paired_sentences = paired_sentences[0]"],"metadata":{"id":"r4eFW-sr2z4f","executionInfo":{"status":"ok","timestamp":1696863418232,"user_tz":-480,"elapsed":4,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["# Assuming tokenL is a list of tokens\n","# Define labels for sentence classification\n","# In this example, each sentence is assigned a label, and 'O' represents 'Other' or 'Not Classified'\n","labels = ['O']  # Initialize labels with 'Other' for the first token\n","current_label = 'O'  # Current label\n","\n","for token in tokenL[1:]:  # Start from the second token since the first token is ''\n","    if token in ['[CLS]', '[SEP]']:\n","        current_label = 'O'  # Reset label for new sentence\n","    else:\n","        labels.append(current_label)  # Assign the current label to the token\n","\n","# Now labels contains a label for each token in your text, including 'O' for tokens not belonging to a sentence"],"metadata":{"id":"zjc7hnHS4Rnp","executionInfo":{"status":"ok","timestamp":1696863419669,"user_tz":-480,"elapsed":5,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["model_path=\"./bert.model\"\n","labels = np.array(tokenL)\n","buildBertModel2(paired_sentences,tokenD,tokenL,model_path)  #tokenL -> labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":497},"id":"oggCAL3u1o3n","executionInfo":{"status":"error","timestamp":1696863478845,"user_tz":-480,"elapsed":14435,"user":{"displayName":"葉佐晨","userId":"06695019314315633787"}},"outputId":"4b1d6a71-df90-4ae3-a271-2832e26571b0"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-85-a2e41efcfc2b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./bert.model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbuildBertModel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaired_sentences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#tokenL -> labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-68-93f5db7b4131>\u001b[0m in \u001b[0;36mbuildBertModel2\u001b[0;34m(paired_sentences, tokenD, tokenL, model_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Tokenize your input data (paired_sentences) and prepare it for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaired_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenL\u001b[0m  \u001b[0;31m# Assuming tokenL contains labels for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2890\u001b[0m                 )\n\u001b[1;32m   2891\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2892\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2893\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3081\u001b[0m         )\n\u001b[1;32m   3082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3083\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3084\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m                 \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids_or_pair_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]}]}