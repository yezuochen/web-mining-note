{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"0bf89465213727ab488c2f290d8a958e943761d5"},"source":["**Natural Language Processing (NLP)** is one of the fastest growing parts of Artificial intelligence. One must have a good command over NLP to process text-based data sets. I recently started on this and after doing some research got to know that below concepts needs to be understood very well before starting a journey on advance NLP computations. Here we will only focus on text preprocessing and feature extraction and later will solve some interesting problem using same."]},{"cell_type":"markdown","metadata":{"_uuid":"93eeb351bec2e5904d870adaecd6e2b8ac5a2db6"},"source":["**Steps -** \n","\n","1. <a href='#import-libs' target='_self'>Importing Libraries</a>\n","1. <a href='#preprocessing' target='_self'>Basics (Preprocessing)</a>\n","    1. <a href='#corpora' target='_self'>NLTK Corpora</a>\n","    1. <a href='#stopwords' target='_self'>Stopwords</a>\n","    1. <a href='#tokenization' target='_self'>Tokenization</a>\n","    1. <a href='#stem-lemma' target='_self'>Stemming & Lemmatization</a>\n","    1. <a href='#post' target='_self'>Part of Speech Tagging</a>\n","1. <a href='#feature-extraction' target='_self'>Feature Extraction (Vectorization)</a>\n","    1. <a href='#bow' target='_self'>Bag of Words</a>\n","    1. <a href='#tf-idf' target='_self'>TF-IDF</a>"]},{"cell_type":"markdown","metadata":{"_uuid":"6f75c81ef8b730dc781e31e862e28618030d052f"},"source":["## <a id='import-libs'>1. Importing Libraries</a>"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in c:\\users\\zuoch\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.0)\n","Requirement already satisfied: pandas in c:\\users\\zuoch\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.1)\n","Requirement already satisfied: numpy>=1.23.2 in c:\\users\\zuoch\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zuoch\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\zuoch\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in c:\\users\\zuoch\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n","Requirement already satisfied: six>=1.5 in c:\\users\\zuoch\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: nltk in c:\\users\\zuoch\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n","Requirement already satisfied: click in c:\\users\\zuoch\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in c:\\users\\zuoch\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zuoch\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2023.8.8)\n","Requirement already satisfied: tqdm in c:\\users\\zuoch\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.1)\n","Requirement already satisfied: colorama in c:\\users\\zuoch\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"]}],"source":["!pip install numpy\n","!pip install pandas\n","!pip install nltk"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['CIER', 'desktop.ini', 'NCCU STAT', 'TAROBO', 'Web概念與技術', '企業倫理與永續發展', '應用迴歸分析', '研究方法（一）', '統計學（二）', '高等數理統計']\n"]}],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","print(os.listdir(\"..\"))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"},"source":["There are many libraries out there like NLTK, TextBlob, SpaCy, Pattern etc that we can use.But, here we are going to prefer NLTK since it is used most commonly and will be good to start with, once we get the grasp of all fundamental operations, we can explore and understand the significance of other libraries too."]},{"cell_type":"code","execution_count":17,"metadata":{"_uuid":"8709f8ffa06a324fb01f1278131776598c6e5ee9","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\zuoch\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     C:\\Users\\zuoch\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# download nltk data 'wordnet' to use lemmatization\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"markdown","metadata":{"_uuid":"691d9742f9f023d4a792809bcaa22bab8b6cc72f"},"source":["## <a id='preprocessing'>2. Basics (Preprocessing)</a>"]},{"cell_type":"markdown","metadata":{"_uuid":"4f4f5371493fbef44e21d5bb603ef9b1a655057b"},"source":["### <a id='corpora'>A. NLTK Corpora</a>\n","One of the best thing about NLTK is that it provides many sample text datasets (Corpora) where each dataset is called Corpus; we can directly import any desired dataset directly from NLTK. Here we are going to import product_reviews data set but you can pick any of the available dataset from http://www.nltk.org/nltk_data/"]},{"cell_type":"code","execution_count":4,"metadata":{"_uuid":"3422fe34d5923bbf4228bb6ac495b293cb4d40c5","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package product_reviews_1 to\n","[nltk_data]     C:\\Users\\zuoch\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package product_reviews_1 is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.corpus import product_reviews_1\n","nltk.download('product_reviews_1')"]},{"cell_type":"markdown","metadata":{"_uuid":"ff5c5a4e5202887b16f89017b47fa979c91daa30"},"source":["Each dataset contains text in text files and to read any file we need to know its name."]},{"cell_type":"code","execution_count":5,"metadata":{"_uuid":"88d970794a3367fb05c6f5ccd7d637dd8ab847f8","trusted":true},"outputs":[{"data":{"text/plain":["['Apex_AD2600_Progressive_scan_DVD player.txt',\n"," 'Canon_G3.txt',\n"," 'Creative_Labs_Nomad_Jukebox_Zen_Xtra_40GB.txt',\n"," 'Nikon_coolpix_4300.txt',\n"," 'Nokia_6610.txt',\n"," 'README.txt']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["product_reviews_1.fileids()"]},{"cell_type":"markdown","metadata":{"_uuid":"090ca623590490f630336ff5006736b1c8491eb3"},"source":["Once we know the file name then we can read from that file in desired way, for eg- "]},{"cell_type":"code","execution_count":6,"metadata":{"_uuid":"07eb40cee16cd17269cde5425e3fbb6f45d6deb1","trusted":true},"outputs":[{"data":{"text/plain":["'*****************************************************************************\\n* Annotated by: Minqing Hu and Bing Liu, 2004.\\n*\\t\\tDepartment of Computer Sicence\\n*               University of Illinois at Chicago              \\n*\\n* Product name: Apex AD2600 Progressive-scan DVD player\\n* Review Source: amazon.com\\n*\\n* See Readme.txt to find the meaning of each symbol. \\n*****************************************************************************\\n\\n[t] troubleshooting ad-2500 and ad-2600 no picture scrolling b/w . \\n##repost from january 13 , 2004 with a better fit title . \\n##does your apex dvd player only play dvd audio without video ? \\n##or does it play audio and video but scrolling in black and white ? \\n##before you try to return the player or was'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Will read raw text from this file\n","product_review_raw = product_reviews_1.raw('Apex_AD2600_Progressive_scan_DVD player.txt')\n","product_review_raw[:750] \n","#We are setting upper limit otherwise it will product the big output with lots of scrolling "]},{"cell_type":"code","execution_count":7,"metadata":{"_uuid":"27e1f1d258908df4a8cebc71a9bb16e9c102ce01","trusted":true},"outputs":[{"data":{"text/plain":["[['repost', 'from', 'january', '13', ',', '2004', 'with', 'a', 'better', 'fit', 'title', '.'], ['does', 'your', 'apex', 'dvd', 'player', 'only', 'play', 'dvd', 'audio', 'without', 'video', '?'], ...]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Will break down file in sentences\n","product_review_sents = product_reviews_1.sents('Apex_AD2600_Progressive_scan_DVD player.txt')\n","product_review_sents"]},{"cell_type":"code","execution_count":8,"metadata":{"_uuid":"32eaf3a8d529ced8abfd22cff3abbd1cbde991aa","trusted":true},"outputs":[{"data":{"text/plain":["['repost', 'from', 'january', '13', ',', '2004', ...]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Will break down file in words\n","product_review_words = product_reviews_1.words('Apex_AD2600_Progressive_scan_DVD player.txt')\n","product_review_words"]},{"cell_type":"markdown","metadata":{"_uuid":"d8e1b389a680ca94dde0a86ecb22c118bd4bded6"},"source":["### <a id='stopwords'>B. Stopwords</a>\n","\n","Stopwords are extra words that don't have any useful meaning they are there just for the sake of sentence formation. They are not really helpful because they can't be categorized, so in NLP projects we prefer their elimination."]},{"cell_type":"code","execution_count":14,"metadata":{"_uuid":"1e3283a9a63418f2aa761b8cdb16b6f3a0d9f692","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}],"source":["from nltk.corpus import stopwords\n","stoplist = stopwords.words('english')\n","print(stoplist)"]},{"cell_type":"markdown","metadata":{"_uuid":"4e42f0d696fe6bef70f973e083b5a52f073d451a","trusted":true},"source":["Let's check the difference between product_reviews_1 length with or without stop_words"]},{"cell_type":"code","execution_count":10,"metadata":{"_uuid":"e311d8a92785620c97262c9f2ab440530d197d56","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["word length with stopwords 12593\n","word length without stopwords 7190\n"]}],"source":["print(f'word length with stopwords {len(product_review_words)}')\n","product_review_wo_stopwords = [word for word in product_review_words if not word in stoplist]\n","print(f'word length without stopwords {len(product_review_wo_stopwords)}')"]},{"cell_type":"markdown","metadata":{"_uuid":"c9d967b2f119c3b1c0eb4672d580ae0b72780ee6"},"source":["We had so many stopwords, so it is somewhat useful to eliminate stopwords before performing any actual NLP operation."]},{"cell_type":"markdown","metadata":{"_uuid":"3de64d653cdaee2f02d69d96a6565c580aca2ea3"},"source":["### <a id='tokenization'>C. Tokenization</a>\n","A 'Token' is nothing but a single entity of whole entity we are referreing to. We can perform sentence and word split in below way:"]},{"cell_type":"code","execution_count":11,"metadata":{"_uuid":"017b5b93eff829128a6e11419d35401e8d05c96a","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Word Tokens - \n","['te hours calling apex tech support , or run the player over with your car , try these simple troubleshooting ideas first .', '##no picture : \\n##hopefully you still have the remote control .', '##if you tossed it out the window , you need to fetch it .', '##using the remote control , press the i/p button located on the bottom right corner of the remote .', '##the i/p button switches the tv display between interlace and progressive .', '##if this doesnt bring back the picture , try pressing this button with']\n","\n","\n","\n","Sentence Tokens - \n","['te', 'hours', 'calling', 'apex', 'tech', 'support', ',', 'or', 'run', 'the', 'player', 'over', 'with', 'your', 'car', ',', 'try', 'these', 'simple', 'troubleshooting', 'ideas', 'first', '.', '#', '#', 'no', 'picture', ':', '#', '#', 'hopefully', 'you', 'still', 'have', 'the', 'remote', 'control', '.', '#', '#', 'if', 'you', 'tossed', 'it', 'out', 'the', 'window', ',', 'you', 'need', 'to', 'fetch', 'it', '.', '#', '#', 'using', 'the', 'remote', 'control', ',', 'press', 'the', 'i/p', 'button', 'located', 'on', 'the', 'bottom', 'right', 'corner', 'of', 'the', 'remote', '.', '#', '#', 'the', 'i/p', 'button', 'switches', 'the', 'tv', 'display', 'between', 'interlace', 'and', 'progressive', '.', '#', '#', 'if', 'this', 'doesnt', 'bring', 'back', 'the', 'picture', ',', 'try', 'pressing', 'this', 'button', 'with']\n"]}],"source":["from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","print(f'Word Tokens - \\n{sent_tokenize(product_review_raw[750:1250])}\\n\\n\\n')\n","print(f'Sentence Tokens - \\n{word_tokenize(product_review_raw[750:1250])}')"]},{"cell_type":"markdown","metadata":{"_uuid":"3ff4fce90391abc988268edeff59055688ca5b43"},"source":["### <a id='stem-lemma'>D. Stemming and Lemmatization</a>\n","They both are used for text normalization. Stemming basically removes the redundancy by bringing everything in its simple form for example 'dancing' & 'dancer' becomes 'dance' in this. On the other hand, Lemmatization does the morphological analysis and keeps part of speech into consideration. This can be better understood by examples :\n","\n","Let's consider below sentence and perform <br/>\n","**Because I had to catch the train, and as we were short on time, I forgot to pack my toothbrush for our vacation.**"]},{"cell_type":"code","execution_count":15,"metadata":{"_uuid":"0be732512bb6bde499965378256a2fcde14bb3f1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Actual Word - A\n","Stem - a\n","Lemma - A\n","\n","Actual Word - middle-aged\n","Stem - middle-ag\n","Lemma - middle-aged\n","\n","Actual Word - woman\n","Stem - woman\n","Lemma - woman\n","\n","Actual Word - entered\n","Stem - enter\n","Lemma - entered\n","\n","Actual Word - the\n","Stem - the\n","Lemma - the\n","\n","Actual Word - room\n","Stem - room\n","Lemma - room\n","\n","Actual Word - ,\n","Stem - ,\n","Lemma - ,\n","\n","Actual Word - her\n","Stem - her\n","Lemma - her\n","\n","Actual Word - hands\n","Stem - hand\n","Lemma - hand\n","\n","Actual Word - full\n","Stem - full\n","Lemma - full\n","\n","Actual Word - of\n","Stem - of\n","Lemma - of\n","\n","Actual Word - hamburger\n","Stem - hamburg\n","Lemma - hamburger\n","\n","Actual Word - meat\n","Stem - meat\n","Lemma - meat\n","\n","Actual Word - as\n","Stem - as\n","Lemma - a\n","\n","Actual Word - she\n","Stem - she\n","Lemma - she\n","\n","Actual Word - formed\n","Stem - form\n","Lemma - formed\n","\n","Actual Word - a\n","Stem - a\n","Lemma - a\n","\n","Actual Word - patty\n","Stem - patti\n","Lemma - patty\n","\n"]}],"source":["from nltk.stem import WordNetLemmatizer\n","from nltk.stem.porter import PorterStemmer\n","sample_sentence = 'A middle-aged woman entered the room, her hands full of hamburger meat as she formed a patty'\n","porter_stemmer = PorterStemmer()\n","word_lemmatizer = WordNetLemmatizer()\n","\n","for w in word_tokenize(sample_sentence):\n","    print(f'Actual Word - {w}')\n","    print(f'Stem - {porter_stemmer.stem(w)}')\n","    print(f'Lemma - {word_lemmatizer.lemmatize(w)}\\n')"]},{"cell_type":"markdown","metadata":{"_uuid":"dd807682605fd31f8fceb4d25557876f778768e2"},"source":["### <a id='post'>E. Part of Speech Tagging</a>\n","Also know as POS Taggin or POST. Why POST requried ? \n","Because same sentence or paragraph can have the same word in different grammatically contexts and it is not a good idea to consider the second occurrence as redundancy, so as a solution we prefer tagging each word with its Part of Speech to make it grammatically unique. Consider below example, here all **above** words are not grammatically same. \n","\n","1. The heavens are **above**. (Adverb)\n","\n","2. The moral code of conduct is **above** the civil code of conduct. (Proposition)\n","\n","3. Our blessings come from **above**. (Noun)"]},{"cell_type":"code","execution_count":18,"metadata":{"_uuid":"dabcc3caaa00bb01cb1cec9d578c6107e46b5a8c","trusted":true},"outputs":[{"data":{"text/plain":["[('A', 'DT'),\n"," ('middle-aged', 'JJ'),\n"," ('woman', 'NN'),\n"," ('entered', 'VBD'),\n"," ('the', 'DT'),\n"," ('room', 'NN'),\n"," (',', ','),\n"," ('her', 'PRP$'),\n"," ('hands', 'NNS'),\n"," ('full', 'JJ'),\n"," ('of', 'IN'),\n"," ('hamburger', 'NN'),\n"," ('meat', 'NN'),\n"," ('as', 'IN'),\n"," ('she', 'PRP'),\n"," ('formed', 'VBD'),\n"," ('a', 'DT'),\n"," ('patty', 'NN')]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["sample_sentence_words = word_tokenize(sample_sentence)\n","nltk.pos_tag(sample_sentence_words)"]},{"cell_type":"markdown","metadata":{"_uuid":"b5cdacbe821a375bd3d56738eea1b19e61f551ab","trusted":true},"source":["## <a id='feature-extraction'>3. Feature Extraction</a>\n","We can not use text directly to train our models. We need to convert it in the form of features, only then it can be used to train any model for desired outcome and we know very well that most of the models respond to the numeric features very well. So we need to bring all these text representations in the form of numbers.\n","\n","There are two popular approaches to extract features from texts: \n","1. Count the number of occurrece of each word in a document. \n","2. Calculate the frequency of each word occurrence out of all word in a document.\n","\n","Few most commonly used techniqus to perform feature extraction are:<br/>\n","**1. Bag of Words**<br/>\n","**2. TF-IDF (Term Frequency - Inverse Document Frequency)**"]},{"cell_type":"markdown","metadata":{"_uuid":"f94d8189392bf84a87254712815949f69e2dd6c1","trusted":true},"source":["### <a id='bow'>A. Bag of Words</a>\n","Bag of words is one of the simplest approaches of feature extraction, here we simply keep the frequency count of all unique words and consider it as a feature. Example: \n","\n","Suppose we have below sentences (also referred as documents):\n","\n","> 1. Must have a subject and a verb.\n","> 2. Must express a complete thought.\n","> 3. Must only have one clause.\n","\n","Feature extraction we need to perform are:\n","\n","**1. Identify Unique words**\n","    Unique words from all documents are:\n","    **must, have, a, subject, and, verb, express, complete, thought, only, one, clause**\n","    \n","**2. Perform Vectorization**\n","    we need to find the frequency count of each unique word and if it is not there then we need to put 0. For eg vector for first document can be formed as: \n","\n","> must - 1 <br/>\n","> have - 1 <br/>\n","> subject - 1 <br/>\n","> and - 1 <br/>\n","> verb - 1 <br/>\n","> express - 0 <br/>\n","> complete - 0 <br/>\n","> thought - 0 <br/>\n","> only - 0 <br/>\n","> one - 0 <br/>\n","> clause - 0 <br/>\n","\n","So, it will become\n","> 1. [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n","\n","similar way doucument2 and document3 will become:\n","> 2. [1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0]\n","> 3. [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n","\n","scikit-learn library provides CountVectorizer class to perform this action"]},{"cell_type":"code","execution_count":22,"metadata":{"_uuid":"8452709810a6a1b8e76726e3184ee193457a608d","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[":: vector vocabulary - {'must': 5, 'have': 4, 'subject': 8, 'and': 0, 'verb': 10, 'express': 3, 'complete': 2, 'thought': 9, 'only': 7, 'one': 6, 'clause': 1}\n","\n",":: vector shape - (3, 11)\n","\n",":: vector list - [[1 0 0 0 1 1 0 0 1 0 1]\n"," [0 0 1 1 0 1 0 0 0 1 0]\n"," [0 1 0 0 1 1 1 1 0 0 0]]\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","# initialize sample document\n","sample_documents = ['Must have a subject and a verb','Must express a complete thought','Must only have one clause']\n","# instantiate\n","vectorizer = CountVectorizer()\n","vectorizer.fit(sample_documents)\n","# summarize\n","print(f':: vector vocabulary - {vectorizer.vocabulary_}\\n')\n","# encode document\n","vector = vectorizer.transform(sample_documents)\n","# summarize encoded vector\n","print(f':: vector shape - {vector.shape}\\n')\n","print(f':: vector list - {vector.toarray()}')"]},{"cell_type":"markdown","metadata":{"_uuid":"d3430d0c85cccb40a3d7835533225ff5bb52754e"},"source":["So if you cross check with our calculated vector list then you will get that both are same, just position is different because of key positions in the dictionary, CountVectorizer lists keys in the dictionary in alphabetical order. \n","\n","This approach is very basic, but has some limitations like it gives importance to words on the basis of their occurrence count, mostly resulting in higher importance to most common and un-important words like 'the', 'is', 'and' etc, so is not very preferred approach for feature extraction. This limitations is handled by TF-IDF method. "]},{"cell_type":"markdown","metadata":{"_uuid":"df2bde0f4d170f3f24ebdf1b3acb91106c9b594a"},"source":["### <a id='tf-idf'>B. Term Frequency – Inverse Document Frequency (TF – IDF)</a>\n","\n","It is the most popular method to perform feature extraction. To understand better let's understand TF and IDF separately.\n","\n","**Term Frequency: **Simply finds out the frequency of a word in document.<br/>\n","**Inverse Document Frequency:** Assigns a lower weight to the words which appear most frequently. It basically depicts the rarity of the word in all documents.\n","\n","![](https://mungingdata.files.wordpress.com/2017/11/equation.png?w=430&h=336) <br/>\n","Similar to CountVectorizer, we can import TfidfVectorizer class from scikit-learn library."]},{"cell_type":"code","execution_count":23,"metadata":{"_uuid":"746e8f9fc48be787a7b12b056ebeadd5d667463b","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[":: vector vocabulary - {'must': 5, 'have': 4, 'subject': 8, 'and': 0, 'verb': 10, 'express': 3, 'complete': 2, 'thought': 9, 'only': 7, 'one': 6, 'clause': 1}\n","\n",":: vector shape - (3, 11)\n","\n",":: vector list - [[0.50461134 0.         0.         0.         0.38376993 0.29803159\n","  0.         0.         0.50461134 0.         0.50461134]\n"," [0.         0.         0.54645401 0.54645401 0.         0.32274454\n","  0.         0.         0.         0.54645401 0.        ]\n"," [0.         0.50461134 0.         0.         0.38376993 0.29803159\n","  0.50461134 0.50461134 0.         0.         0.        ]]\n"]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","# initialize sample document\n","sample_documents = ['Must have a subject and a verb','Must express a complete thought','Must only have one clause']\n","# instantiate\n","vectorizer = TfidfVectorizer()\n","vectorizer.fit(sample_documents)\n","# summarize\n","print(f':: vector vocabulary - {vectorizer.vocabulary_}\\n')\n","# encode document\n","vector = vectorizer.transform(sample_documents)\n","# summarize encoded vector\n","print(f':: vector shape - {vector.shape}\\n')\n","print(f':: vector list - {vector.toarray()}')"]},{"cell_type":"markdown","metadata":{"_uuid":"4f269a4447d4123fb68602192efd686bde626636"},"source":["**Interpretation: **\n","\n","dictionary - \n","> {'and': 0, 'clause': 1, 'complete': 2, 'express': 3, 'have': 4, 'must': 5, 'one': 6, 'only': 7, 'subject': 8, 'thought': 9, 'verb': 10} <br/>\n","\n","document 1 - \n","> 'Must have a subject and a verb'<br/>\n","\n","vector - \n","> [0.50461134  0.  0.  0.  0.38376993  0.29803159  0.  0.  0.50461134  0.  0.50461134]\n","\n","> and - 0.50461134<br/>\n","> clause - 0.<br/>\n","> complete - 0. <br/>\n","> express - 0.<br/>\n","> have - 0.38376993<br/>\n","> must - 0.29803159<br/>\n","> one - 0. <br/>\n","> only - 0. <br/>\n","> subject - 0.50461134<br/>\n","> thought - 0. <br/>\n","> verb - 0.50461134<br/>\n","\n","Excluding 0, 'must' have the lowest weight because it is most frequent in all documents and this is what IDF does."]},{"cell_type":"markdown","metadata":{"_uuid":"fcb974d17b9ab0f400ca147ae4e0d68578752f4a"},"source":["All these concepts can be grasped better while working on the actual problem. This is it for now ;)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
